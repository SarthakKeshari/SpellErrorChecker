{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpellingErrorChecker.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMr2CU01fqJl6YUNYs25VwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarthakKeshari/SpellErrorChecker/blob/main/SpellingErrorChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1rmrig1Lba2",
        "outputId": "dc78b49e-5c2c-4c18-ffb3-b566f6f2f877"
      },
      "source": [
        "#Finding spelling error in a sentence\n",
        "\n",
        "# !apt install enchant\n",
        "# !pip install pyenchant\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker\n",
        "from enchant.tokenize import EmailFilter, URLFilter\n",
        "from enchant.tokenize import get_tokenizer\n",
        "# import language_check\n",
        "\n",
        "\n",
        "checker = SpellChecker(\"en_US\",filters=[EmailFilter,URLFilter])\n",
        "dictionary = enchant.Dict(\"en_US\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#load data\n",
        "text = \"Hello, My name Sarthak is, srthk@gmail.com\"\n",
        "\n",
        "#Checking spelling\n",
        "checker.set_text(text)\n",
        "for err in checker:\n",
        "  # if(nlp(err.word)[0].pos_!=\"PROPN\"):\n",
        "  print (\"ERROR:\", err.word)\n",
        "  print (\"SUGGESTIONS to rectify error: \",dictionary.suggest(err.word))\n",
        "  print()\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR: Sarthak\n",
            "SUGGESTIONS to rectify error:  ['Siddhartha']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVbcqZ7nNGTT",
        "outputId": "6a646e28-4686-4c34-8bf4-5d542aeca12a"
      },
      "source": [
        "#Finding spelling errors in a data.txt file\n",
        "\n",
        "# !apt install enchant\n",
        "# !pip install pyenchant\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker\n",
        "from enchant.tokenize import EmailFilter, URLFilter\n",
        "from enchant.tokenize import get_tokenizer\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "checker = SpellChecker(\"en_US\",filters=[EmailFilter,URLFilter])\n",
        "dictionary = enchant.Dict(\"en_US\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "file_location_data = '/drive/My Drive/Colab Notebooks/data.txt'\n",
        "file_location_error = '/drive/My Drive/Colab Notebooks/error.txt'\n",
        "file_location_ignore = '/drive/My Drive/Colab Notebooks/ignore.txt'\n",
        "\n",
        "ignore_list=[]\n",
        "with open(file_location_ignore) as f:\n",
        "  lines = f.readline()\n",
        "  while(lines!=\"\"):\n",
        "    tokenizer = get_tokenizer(\"en_US\")\n",
        "    token = [w for w in tokenizer(lines)]\n",
        "    ignore_list.append(token[0][0])\n",
        "    lines = f.readline()\n",
        "\n",
        "# print(ignore_list)\n",
        "\n",
        "o_error = open(file_location_error, 'w')\n",
        "\n",
        "line_number = 1\n",
        "with open(file_location_data) as f:\n",
        "  lines = f.readline()\n",
        "  while(lines!=\"\"):\n",
        "    # print(lines)\n",
        "    #Checking spelling\n",
        "    checker.set_text(lines)\n",
        "    for err in checker:\n",
        "      if err.word not in ignore_list:\n",
        "        # print(err.word)\n",
        "        o_error.write(\"ERROR: \"+ err.word+\"\\n\")\n",
        "        o_error.write(\"Error occurs on - \\n\")\n",
        "        o_error.write(\"LINE NO.: \"+str(line_number)+\"\\n\")\n",
        "        suggest_str=\"\"\n",
        "        for ele in dictionary.suggest(err.word):\n",
        "          suggest_str += ele + \" \"\n",
        "        o_error.write(\"SUGGESTIONS to rectify error: \"+ suggest_str+\"\\n\\n\")\n",
        "    line_number+=1;\n",
        "    lines = f.readline()\n",
        "\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGAwCY1Kr5oo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXTvlzbpQjTd",
        "outputId": "02468cb4-34b1-4cd9-b79d-8de6e053fec0"
      },
      "source": [
        "#Finding spelling errors in a data.pdf file\n",
        "\n",
        "# !apt install enchant\n",
        "# !pip install pyenchant\n",
        "!pip install PyPDF2\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker\n",
        "from enchant.tokenize import EmailFilter, URLFilter\n",
        "from enchant.tokenize import get_tokenizer\n",
        "import os\n",
        "from google.colab import drive\n",
        "import PyPDF2\n",
        "\n",
        "drive.mount('/drive')\n",
        "\n",
        "checker = SpellChecker(\"en_US\",filters=[EmailFilter,URLFilter])\n",
        "dictionary = enchant.Dict(\"en_US\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "file_location_data = '/drive/My Drive/Colab Notebooks/data.pdf'\n",
        "file_location_error = '/drive/My Drive/Colab Notebooks/error.txt'\n",
        "file_location_ignore = '/drive/My Drive/Colab Notebooks/ignore.txt'\n",
        "\n",
        "# pdf file object\n",
        "# you can find find the pdf file with complete code in below\n",
        "pdfFileObj = open(file_location_data, 'rb')\n",
        "# pdf reader object\n",
        "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "# number of pages in pdf\n",
        "print(pdfReader.numPages)\n",
        "# a page object\n",
        "pageObj = pdfReader.getPage(0)\n",
        "# extracting text from page.\n",
        "# this will print the text you can also save that into String\n",
        "print(pageObj.extractText())\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "1\n",
            "Sam, Ram and Dhani learning that Java is a programmg languge and a platfom. Java is a hih level, \n",
            "robust, obje\n",
            "-\n",
            "oriented and secure programng languge.\n",
            " \n",
            " \n",
            "Java was devlped by Sun Microsystems (which is now the subsidiary of Oracle) in the year 1995. \n",
            "James Gosling is known as the father of Java. Before Java, its nme was Oak. Since Oak was already a \n",
            "registered company, so James Gosling and his team chnged the \n",
            "Oak name to Java.\n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ8-v6lIsxMN",
        "outputId": "7639a536-9964-47a1-8e79-bc873382f8de"
      },
      "source": [
        "#Finding spelling errors in a data.docx file and highlighting the same\n",
        "\n",
        "# !apt install enchant\n",
        "# !pip install pyenchant\n",
        "# !pip install python-docx \n",
        "\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import enchant\n",
        "from enchant.checker import SpellChecker\n",
        "from enchant.tokenize import EmailFilter, URLFilter\n",
        "from enchant.tokenize import get_tokenizer\n",
        "import os\n",
        "import docx\n",
        "from docx.shared import RGBColor\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "checker = SpellChecker(\"en_US\",filters=[EmailFilter,URLFilter])\n",
        "dictionary = enchant.Dict(\"en_US\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "file_location_data = '/drive/My Drive/Colab Notebooks/data.docx'\n",
        "file_location_copy = '/drive/My Drive/Colab Notebooks/copy.docx'\n",
        "file_location_error = '/drive/My Drive/Colab Notebooks/error.docx'\n",
        "file_location_ignore = '/drive/My Drive/Colab Notebooks/ignore.docx'\n",
        "\n",
        "data_file = docx.Document(file_location_data)\n",
        "error_file = docx.Document()\n",
        "ignore_file = docx.Document(file_location_ignore)\n",
        "copy_file = docx.Document()  # To highlight the error\n",
        "\n",
        "ignore_list=[]\n",
        "for para in ignore_file.paragraphs:\n",
        "  tokenizer = get_tokenizer(\"en_US\")\n",
        "  token = [w for w in tokenizer(para.text)]\n",
        "  if(len(token)!=0):\n",
        "    ignore_list.append(token[0][0].lower())\n",
        "\n",
        "# print(ignore_list)\n",
        "\n",
        "err_word=0\n",
        "err_word_highlighted=0\n",
        "\n",
        "# Checking spelling error\n",
        "para_number = 1\n",
        "for para in data_file.paragraphs:\n",
        "  if(len(para.text)!=0):\n",
        "    checker.set_text(para.text)\n",
        "    for err in checker:\n",
        "      if err.word.lower() not in ignore_list:\n",
        "        write_para = error_file.add_paragraph()\n",
        "        # print(err.word)\n",
        "        write_para.add_run(\"ERROR: \"+ err.word+\"\\n\")\n",
        "        write_para.add_run(\"Error occurs on - \\n\")\n",
        "        write_para.add_run(\"PARAGRAPH NO.: \"+str(para_number)+\"\\n\")\n",
        "        suggest_str=\"\"\n",
        "        for ele in dictionary.suggest(err.word):\n",
        "          suggest_str += ele + \" \"\n",
        "        write_para.add_run(\"SUGGESTIONS to rectify error: \"+ suggest_str+\"\\n\\n\")\n",
        "\n",
        "        err_word+=1\n",
        "    para_number+=1;\n",
        "\n",
        "error_file.save(file_location_error)\n",
        "\n",
        "# Creating copied file\n",
        "for para in data_file.paragraphs:\n",
        "  write_para = copy_file.add_paragraph()\n",
        "\n",
        "  if(len(para.text)!=0):\n",
        "    tokens = word_tokenize(para.text)\n",
        "    checker.set_text(para.text)\n",
        "    error_words = [err.word.lower() for err in checker]\n",
        "    for token in tokens:\n",
        "      # print(token)\n",
        "      if token not in error_words:\n",
        "        write_para.add_run(token+\" \")\n",
        "      else:\n",
        "        if token in ignore_list:\n",
        "          write_para.add_run(token+\" \")\n",
        "        else:\n",
        "          write_para.add_run(token+\" \").font.color.rgb = RGBColor(0xee, 0x00, 0x00)\n",
        "          err_word_highlighted+=1\n",
        "          # print(token)\n",
        "\n",
        "copy_file.save(file_location_copy)\n",
        "\n",
        "# Accuracy score in formatting error words in original doc\n",
        "# Number of error words successfully formatted in original docx/Number of error words caught by error.docx\n",
        "\n",
        "print(\"Accuracy: \"+str(err_word_highlighted/err_word))\n",
        "\n",
        "print(\"SUCCESSFUL PROGRAM COMPLETION\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "Accuracy: 0.8\n",
            "SUCCESSFUL PROGRAM COMPLETION\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4aqByolyoMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}